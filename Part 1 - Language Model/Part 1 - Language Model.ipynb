{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Building a Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Francesca Maria Mizzi - 118201L\n",
    "#### B.Sc in IT (Honours) (Artificial Intelligence)\n",
    "#### ICS2203 - Natural Language Processing: Methods and Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following Jupyter Notebook, I will be creating 3 different types on N-gram models based on the (Baby) British National Corpus; vanilla language model, laplace language model and UNK language model. The n-grams I will use are unigrams, bigrams and trigrams. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in order to create the language model is importing all the relevant modules. As you can see, I mostly made use of NLTK in order to manipulate the corpus however I also used sklearn in order to split my dataset to train and test as well as numpy, random, mpmath and sys in order to generate the probabilities, perplexity and the sentence generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.bnc import BNCCorpusReader\n",
    "from nltk.collocations import *\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.lm import MLE\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import nltk, re, pprint, string\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import numpy\n",
    "import random\n",
    "import mpmath as mp\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import os\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen below, I made use of the NLTK module BNCCorpusReader. This module doesn't do too much other than reading the files specified in the fileids variable and parsing the text within the xml file.\n",
    "\n",
    "Due to the way the module is initialized, it is very easy to include more files from within the (Baby) British National Corpus simply by adding them to the list fileids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did not submit the whole (Baby) British National Corpus but only the files which I used below. Should you wish to add more files from the corpus, they need to be downloaded and put in the appropriate location as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnc_reader = BNCCorpusReader(root=\"BNC/Texts\", fileids=r'[A-K]/\\w*/\\w*\\.xml')\n",
    "fileids = ['aca/A6U.xml', 'aca/ACJ.xml']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building the language models, the corpus must first be pre-processed. Pre-processing allows us to filter out any symbols or phrases which are not desired within the corpus.\n",
    "\n",
    "As can be seen, I use the BNCCorpusReader in order to extract the raw sentences from the corpus. I then loop through each sentence word by word and check if there are any symbols within the word. If symbols are present, that symbol is removed and the new word appended to the list of tokens. If there are no symbols within the word, the word is automatically added to the list of tokens.\n",
    "\n",
    "It is also to be noted that before looping through the words of a sentence, the start of sentence token is added to indicate the beginning of a new sentence. When it has looped through an entire sentence, it appends the end of sentence token to the list in order to indicate the end of a sentence. These tokens are especially important when generating the sentences later on. \n",
    "\n",
    "After removing all the symbols, all the characters are made lowercase in order to facilitate word prediction and sentence generation later on since the language model does not consider a word with a lowercase character and an uppercase character to be the same. For example, to the language model, \"the\" is not the same as \"The\" since the first letter is uppercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sents = BNCCorpusReader.sents(bnc_reader, fileids=fileids)\n",
    "punct = \"“”‘’!\\\"#$€%&()*'+-,./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "temp = []\n",
    "        \n",
    "for sentence in raw_sents:\n",
    "    temp.append(\"<s>\")\n",
    "    for word in sentence:\n",
    "        for letter in word:\n",
    "            if letter in punct:\n",
    "                word.replace(letter,'')\n",
    "        temp.append(word)\n",
    "    temp.append(\"</s>\")\n",
    "    \n",
    "tokens = [x.lower() for x in temp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then split the corpus tokens into test and train sets using the module from sklearn. In my case, I chose to make the test set 20% of the total corpus but that can easily be changed by adjusting the second variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words, test_words = train_test_split(tokens, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now begins the building of the language models. I build 3 different types of models: vanilla (a basic language model), laplace (+1 smoothing added to the vanilla model) and UNK model (replacing tokens with a frequency less than 2 by UNK tokens and using the laplace model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make my code as efficient as possible, I tried to code as modular as possible in order to easily call specific methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the vanilla model, I created 3 different functions, unigram, bigram and trigram models.\n",
    "\n",
    "For the unigram model, I use the counter module to count each instance of each word within the training set. Then, for every word within the new model, I take the instance count and divide it by the total number of words within the training set.\n",
    "\n",
    "For the bigram model, I first found all the bigrams within the corpus by taking each word, finding the word before it and adding the tuple to a list of bigrams. The same process as the unigram model is performed but this time dividing instead with the instance count for the first word (unigram) of the bigram.\n",
    "\n",
    "The trigram model is very similar to the bigram model where the bigrams and trigrams are found by find the word before each word and using the count of each trigran and dividing it by the bigram count of the first two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_uni(train_words):\n",
    "    unigram = Counter(train_words)\n",
    "    \n",
    "    for word in unigram:\n",
    "        unigram[word] = unigram[word]/len(train_words)\n",
    "        \n",
    "    return unigram\n",
    "def vanilla_bi(train_words):\n",
    "    bigram = Counter([(word, train_words[i + 1]) for i, word in enumerate(train_words[:-1])])\n",
    "    counter = Counter(train_words)\n",
    "    \n",
    "    for word in bigram:\n",
    "        bigram[word] = bigram[word]/counter[word[0]]\n",
    "        \n",
    "    return bigram\n",
    "    \n",
    "def vanilla_tri(train_words):\n",
    "    bigram = Counter([(word, train_words[i + 1]) for i, word in enumerate(train_words[:-1])])\n",
    "    trigram = Counter([(word, train_words[i + 1], train_words[i + 2]) for i, word in enumerate(train_words[:-2])])\n",
    "    \n",
    "    for word in trigram:\n",
    "        trigram[word] = trigram[word]/bigram[(word[0], word[1])]\n",
    "        \n",
    "    return trigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The laplace models are almost identical to the vanilla models, the only difference being that before dividing, 1 is added to the numerator in order to perform +1 smoothing. Smoothing is performed in order to generalize better. \n",
    "\n",
    "As done prior, 3 different methods were created for each of the n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_uni(train_words):\n",
    "    unigram = Counter(train_words)\n",
    "    \n",
    "    for word in unigram:\n",
    "        unigram[word] = unigram[word]+1/len(train_words)\n",
    "        \n",
    "    return unigram\n",
    "def laplace_bi(train_words):\n",
    "    bigram = Counter([(word, train_words[i + 1]) for i, word in enumerate(train_words[:-1])])\n",
    "    counter = Counter(train_words)\n",
    "    \n",
    "    for word in bigram:\n",
    "        bigram[word] = bigram[word]+1/counter[word[0]]\n",
    "        \n",
    "    return bigram\n",
    "    \n",
    "def laplace_tri(train_words):\n",
    "    bigram = Counter([(word, train_words[i + 1]) for i, word in enumerate(train_words[:-1])])\n",
    "    trigram = Counter([(word, train_words[i + 1], train_words[i + 2]) for i, word in enumerate(train_words[:-2])])\n",
    "    \n",
    "    for word in trigram:\n",
    "        trigram[word] = trigram[word]+1/bigram[(word[0], word[1])]\n",
    "        \n",
    "    return trigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNK Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UNK language models are based very heavily on the laplace models mentioned prior. How the UNK model works is as follows:\n",
    " - Getting the frequency counts for each word in the corpus\n",
    " - If the frequency count for that word is less than or equal to 2, that word is \"removed\" from the corpus and replaced by  the UNK token\n",
    " - The laplace is model is then applied to this new training set\n",
    " \n",
    " In the case of the bigram and trigram UNK models, the unigram UNK model is first created in order to check and replace each individual word and then passes it through the laplace bigram or trigram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_uni(train_words):\n",
    "    \n",
    "    counter = Counter(train_words)\n",
    "    model = {}\n",
    "    model[\"<UNK>\"] = 0\n",
    "    \n",
    "    for word in counter:\n",
    "        if counter[word] <= 2:\n",
    "            model[\"<UNK>\"] += 1\n",
    "            \n",
    "        else:\n",
    "            model[word] = counter[word]\n",
    "            \n",
    "    for word in model:\n",
    "        model[word] = model[word]+1/len(train_words)\n",
    "        \n",
    "    return model\n",
    "\n",
    "def unk_bi(train_words):\n",
    "    \n",
    "    unigram = unk_uni(train_words)\n",
    "    \n",
    "    for i, word in enumerate(train_words):\n",
    "        if not (word in unigram):\n",
    "            train_words[i] = \"<UNK>\"\n",
    "            \n",
    "    return laplace_bi(train_words)\n",
    "\n",
    "def unk_tri(train_words):\n",
    "    \n",
    "    unigram = unk_uni(train_words)\n",
    "    \n",
    "    for i, word in enumerate(train_words):\n",
    "        if not (word in unigram):\n",
    "            train_words[i] = \"<UNK>\"\n",
    "            \n",
    "    return laplace_tri(train_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to carry out linear interpolation and calculate the probability of a sentence existing, we must first figure out the probability of the existence of each unigram, bigram and trigram within the sentence. In order to do this, I created 3 methods which calculate the probability of a specific n-gram. \n",
    "\n",
    "The probability of a unigram is called by calling the specific entry of that unigram from the specified language model.\n",
    "\n",
    "The probability of a bigram is called by splitting up the bigram into 2 words and searching for that bigram within the language model.\n",
    "\n",
    "The probability of a trigram is done like the probability of a bigram but instead splitting the n-gram into 3 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uni_prob(model,unigram):\n",
    "    probability = model[unigram]\n",
    "    return probability\n",
    "\n",
    "def bi_prob(model,bigram):\n",
    "    first = bigram.split()[0]\n",
    "    second = bigram.split()[1]\n",
    "    probability = model[first,second]\n",
    "    return probability\n",
    "\n",
    "def tri_prob(model, trigram):\n",
    "    first = trigram.split()[0]\n",
    "    second = trigram.split()[1]\n",
    "    third = trigram.split()[2]\n",
    "    probability = model[first,second,third]\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the probabilty of a sentence existing using our language models, I carried out linear interpolation on the sentence. This is done by taking the probability of the unigram, bigrams and trigrams, multiplying them by their individual lambdas and adding them together. \n",
    "\n",
    "However, it is very clear that in most sentences, multiple unigrams, bigrams and trigrams exist. Therefore, to find the probability of each of the different types on n-grams, I used the chain rule of probability. This chain rule implies that multiplying the probabilities of each unigram gives the total unigram probability, multiplying the probabilites of each bigram gives the total bigram probability, etc. etc.\n",
    "\n",
    "The calculation of the probability of a sentence existing is as follows:\n",
    " - The sentence is padded with the start and end tokens in order to accuratly find the frequency counts within the corpus\n",
    " - The sentence is split up into words and added to a list\n",
    " - The individual lambdas for the linear interpolation are declared\n",
    " - For the unigram probability of the sentence, the individual probability of each word in the sentence is found and appended to a list of probabilities\n",
    " - For the bigram probability of the sentence, all of the bigrams within the sentence are found. The probability of each bigram is then found and appeneded to a list of probabilities\n",
    " - For the trigram probability of the sentence, all of the trigrams within the sentence are found. The probability of each trigram is then found and appended to a list of probabilities\n",
    " \n",
    "The above steps are performed on each different type of language model, depending on that is specified.\n",
    "\n",
    "Finally, in order to carry out the linear interpolation, the total unigram, bigram and trigram probabilities are calculated by multiplying the values within each list, respectively. We take these final 3 probabilities, multiply them by their respective lambdas and add them together. The result is the probability of a sentence existing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def probability (sentence, model):\n",
    "    sent = \"<s> \"+ sentence + \" </s>\"\n",
    "    words = sent.split()\n",
    "    uni_lambda = 0.1\n",
    "    bi_lambda = 0.3\n",
    "    tri_lambda = 0.6\n",
    "    \n",
    "    unigrams_probability = []\n",
    "    bigrams_probability = []\n",
    "    trigrams_probability = []\n",
    "    \n",
    "    if model == \"Vanilla\":\n",
    "        # unigram\n",
    "                \n",
    "        for word in words:\n",
    "            unigrams_probability.append(uni_prob(vanilla_uni(train_words),word))\n",
    "        \n",
    "        # bigram\n",
    "        \n",
    "        bigrams = nltk.ngrams(words, 2)\n",
    "        for pair in bigrams:\n",
    "            bigram = ' '.join(pair)\n",
    "            bigrams_probability.append(bi_prob(vanilla_bi(train_words), vanilla_uni(train_words), bigram))\n",
    "        \n",
    "        # trigram\n",
    "        trigrams = nltk.ngrams(words, 3)\n",
    "        for trio in trigrams:\n",
    "            trigram = ' '.join(trio)\n",
    "            trigrams_probability.append(tri_prob(vanilla_tri(train_words),vanilla_bi(train_words),trigram))\n",
    "        \n",
    "    elif model == \"Laplace\":\n",
    "        # unigram\n",
    "                \n",
    "        for word in words:\n",
    "            unigrams_probability.append(uni_prob(laplace_uni(train_words),word))\n",
    "        \n",
    "        # bigram\n",
    "        \n",
    "        bigrams = nltk.ngrams(words, 2)\n",
    "        for pair in bigrams:\n",
    "            bigram = ' '.join(pair)\n",
    "            bigrams_probability.append(bi_prob(laplace_bi(train_words), laplace_uni(train_words), bigram))\n",
    "        \n",
    "        # trigram\n",
    "        trigrams = nltk.ngrams(words, 3)\n",
    "        for trio in trigrams:\n",
    "            trigram = ' '.join(trio)\n",
    "            trigrams_probability.append(tri_prob(laplace_tri(train_words),laplace_bi(train_words),trigram))\n",
    "        \n",
    "    elif model == \"UNK\":\n",
    "        # unigram\n",
    "                \n",
    "        for word in words:\n",
    "            unigrams_probability.append(uni_prob(unk_uni(train_words),word))\n",
    "        \n",
    "        \n",
    "        # bigram\n",
    "        \n",
    "        bigrams = nltk.ngrams(words, 2)\n",
    "        for pair in bigrams:\n",
    "            bigram = ' '.join(pair)\n",
    "            bigrams_probability.append(bi_prob(unk_bi(train_words), unk_uni(train_words), bigram))\n",
    "        \n",
    "        # trigram\n",
    "        trigrams = nltk.ngrams(words, 3)\n",
    "        for trio in trigrams:\n",
    "            trigram = ' '.join(trio)\n",
    "            trigrams_probability.append(tri_prob(unk_tri(train_words),unk_bi(train_words),trigram))\n",
    "        \n",
    "    prob1 = numpy.prod(unigrams_probability)\n",
    "    prob2 = numpy.prod(bigrams_probability)\n",
    "    prob3 = numpy.prod(trigrams_probability) \n",
    "    \n",
    "    probability = (uni_lambda*prob1)+(bi_lambda*prob2)+(tri_lambda*prob3)\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate the perplexity of the sentence, I made use of the mpmath unit. This unit was used in order to hold the perplexity in a true float.\n",
    "\n",
    "In order to calculate the perplexity, I took each line from my test set and stored the length of each line in a variable N. I then combined each line to form a sentence. I then checked if the sentence exits in my model. If it does, I multiply the current perplexity p with the inverse of the frequence of that line. If the sentence does not exist in the mdoel, the perplexity is multiplied by a very large number (sys.maxsize). The final perplexity is calculated by calculating the current perplexity p to the power of the inverse of the number of lines.\n",
    "\n",
    "I am aware that I did not do this part correctly as I split up the test and train corpus by word and not by sentence so the perplexity is not accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(test_words, model):\n",
    "    \n",
    "    p = mp.mpf(1)\n",
    "    \n",
    "    N = mp.mpf(0)\n",
    "    \n",
    "    for line in test_words:\n",
    "        N += len(line)\n",
    "        line = ' '.join(line)\n",
    "        \n",
    "        if model[line] > 0:\n",
    "            p = p * (1/model[line])\n",
    "        else:\n",
    "            p = p * sys.maxsize\n",
    "            \n",
    "    p = pow(p, 1/float(N))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mpf('15694.328977677425')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(test_words, vanilla_bi(train_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generate the next word of a sentence, I created three methods which generates the next word based on the 3 different type on n-gram models, unigram, bigram and trigram. \n",
    "\n",
    "The unigram generation is done as follows:\n",
    "- Creating a numpy array with the values of all the unigrams in the respective model\n",
    "- Finding the average of this weight\n",
    "- Choosing a word semi-randomly and getting the index and weight of this random word\n",
    "- Adding this new word to the sentence\n",
    "This is done until the new word generated is the stop token.\n",
    "\n",
    "The bigram generation is done very similarly to the unigram generation with the only difference being that it takes the last two words of a sentence rather than only 1.\n",
    "\n",
    "Unfortunately, I did not manage to make my trigram generation work. For some reason, it chooses to keep the original sentence and does not generate any new words. \n",
    "\n",
    "\n",
    "My original plan for the bigram and trigram generation was to find the probability of each bigram and trigram based on the last 1 or 2 words of the sentence, respectively, and generate the next word based on the bigram or trigram which has the highest probability. However, I quickly discovered that this took far too long to run and choose to instead take a simpler approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uni_generate(model, sentence, last = \"\", count = None):\n",
    "    \n",
    "    if(count != 0 and sentence[-1] != last):\n",
    "        \n",
    "        weights = numpy.array(list(model.values()))\n",
    "        norm = weights/numpy.sum(weights)\n",
    "        \n",
    "        resample = numpy.random.multinomial(1, norm)\n",
    "        key = list(resample).index(1)\n",
    "        value = list(model.keys())[key]\n",
    "        \n",
    "        sentence.append(value)\n",
    "        if count != None:\n",
    "            uni_generate(model, sentence, last, count-1)\n",
    "        else:\n",
    "            uni_generate(model, sentence, last)\n",
    "            \n",
    "    return sentence\n",
    "\n",
    "def bi_generate(model, sentence, last, count = None):\n",
    "    if(count != 0 and sentence[-1] != last):\n",
    "        \n",
    "        bigrams = []\n",
    "        last_word = sentence[-1]\n",
    "        \n",
    "        \n",
    "        for entry in model:\n",
    "            if entry[0] == last_word:\n",
    "                bigrams.append((entry,model[entry]))\n",
    "        if(bigrams == []):\n",
    "            return sentence \n",
    "        \n",
    "        v = [x[1] for x in bigrams]\n",
    "        k = [x[0] for x in bigrams]\n",
    "        weights = numpy.array(v)\n",
    "        norm = weights / numpy.sum(weights)\n",
    "        resample = numpy.random.multinomial(1, norm)\n",
    "        key = list(resample).index(1)\n",
    "        value = k[key]\n",
    "\n",
    "        sentence.append(value[1])\n",
    "\n",
    "        if count != None:\n",
    "            bi_generate(model, sentence, last, count-1)\n",
    "        else:\n",
    "            bi_generate(model, sentence, last)\n",
    "        \n",
    "    return sentence\n",
    "\n",
    "\n",
    "def tri_generate(bi_model, tri_model, sentence, last = \"\", count = None):\n",
    "    if(len(sentence) == 1):\n",
    "        sentence = BigramGenerate(bi_model, sentence, last, count=1)\n",
    "        \n",
    "    if(count != 0 and sentence[-1] != last):\n",
    "        trigrams = []\n",
    "        last_word = sentence[-1]\n",
    "        \n",
    "        for entry in tri_model:\n",
    "            if(entry[0] == sentence[-2] and entry[1] == sentence[-1]):\n",
    "                trigrams.append((entry,tri_model[entry]))\n",
    "                \n",
    "        if(trigrams == []):\n",
    "            return sentence\n",
    "        \n",
    "        v = [x[1] for x in trigrams]\n",
    "        k = [x[0] for x in trigrams]\n",
    "        weights = np.array(v)\n",
    "        norm = weights / np.sum(weights)\n",
    "        resample = np.random.multinomial(1, norm)\n",
    "        key = list(resample).index(1)\n",
    "        value = k[key] \n",
    "        \n",
    "        sentence.append(value[2])\n",
    "        if count != None:\n",
    "            tri_generate(bi_model, tri_model, sentence, last, count-1)\n",
    "            \n",
    "        else:\n",
    "            tri_generate(bi_model, tri_model, sentence, last)\n",
    "            \n",
    "            \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I created a function used to generate all 3 types of sentences based on the methods created above. This method checks to see which model is selected and calls the relevant language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, phrase):\n",
    "    if model == \"1\" or model == \"Vanilla\" or model == \"vanilla\":\n",
    "        sent1 = [\"<s>\"]\n",
    "        w = phrase.split()\n",
    "        for word in w:\n",
    "            sent1.append(word)\n",
    "\n",
    "        sent2 = sent1.copy()\n",
    "        sent3 = sent1.copy()\n",
    "\n",
    "        print(\"Generating vanilla model...\")\n",
    "        print(\"GENERATED VANILLA SENTENCES\")\n",
    "        print(\"Unigram: \"+ str (uni_generate(model = vanilla_uni(train_words), sentence = sent1, last = \"</s>\")))\n",
    "        print(\"Bigram: \"+ str (bi_generate(model = vanilla_bi(train_words), sentence = sent2, last = \"</s>\")))\n",
    "        print(\"Trigram: \"+ str (tri_generate(bi_model = vanilla_bi(train_words), tri_model = vanilla_tri(train_words), sentence = sent3, last = \"</s>\")))\n",
    "        \n",
    "    elif model == \"2\" or model == \"Laplace\" or model == \"laplace\":\n",
    "        sent1 = [\"<s>\"]\n",
    "        w = phrase.split()\n",
    "        for word in w:\n",
    "            sent1.append(word)\n",
    "\n",
    "        sent2 = sent1.copy()\n",
    "        sent3 = sent1.copy()\n",
    "\n",
    "        print(\"Generating laplace model...\")\n",
    "        print(\"GENERATED LAPLACE SENTENCES\")\n",
    "        print(\"Unigram: \"+ str (uni_generate(model = laplace_uni(train_words), sentence = sent1, last = \"</s>\")))\n",
    "        print(\"Bigram: \"+ str (bi_generate(model = laplace_bi(train_words), sentence = sent2, last = \"</s>\")))\n",
    "        print(\"Trigram: \"+ str (tri_generate(bi_model = laplace_bi(train_words), tri_model = laplace_tri(train_words), sentence = sent3, last = \"</s>\")))\n",
    "        \n",
    "    elif model == \"3\" or model == \"UNK\" or model == \"unk\":\n",
    "        sent1 = [\"<s>\"]\n",
    "        w = phrase.split()\n",
    "        for word in w:\n",
    "            sent1.append(word)\n",
    "\n",
    "        sent2 = sent1.copy()\n",
    "        sent3 = sent1.copy()\n",
    "\n",
    "        print(\"Generating UNK model...\")\n",
    "        print(\"GENERATED UNK SENTENCES\")\n",
    "        print(\"Unigram: \"+ str (uni_generate(model = unk_uni(train_words), sentence = sent1, last = \"</s>\")))\n",
    "        print(\"Bigram: \"+ str (bi_generate(model = unk_bi(train_words), sentence = sent2, last = \"</s>\")))\n",
    "        print(\"Trigram: \"+ str (tri_generate(bi_model = unk_bi(train_words), tri_model = unk_tri(train_words), sentence = sent3, last = \"</s>\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code used in order to generate sentences based off the users input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which model would you like to use? (1) Vanilla (2) Laplace (3) UNK : 1\n",
      "Enter phrase to continue to generate: they all went\n",
      "Generating vanilla model...\n",
      "GENERATED VANILLA SENTENCES\n",
      "Unigram: ['<s>', 'they', 'all', 'went', 'includes', 'dangerous', '</s>']\n",
      "Bigram: ['<s>', 'they', 'all', 'went', 'of', '.', 'and', '<UNK>', 'social', 'thus', 'law', ',', 'adult', 'the', 'in', 'murder', '<s>', '<UNK>', 'should', 'it', 'violence', '<s>', 'test', 'the', 'granted', 'there', 'offence', 'is', 'he', '<s>', ',', 'noticed', 'considerably', '<UNK>', 'in', '.', '.', 'criminal', '‘', 'provocation', 'in', ',', 'who', 'variety', 'juan', 'for', '.', 'the', 'force', 'still', 'bad', 'in', 'care', 'of', '—', 'driving', 'much', 'the', '<UNK>', 'citizen', 'on', '<UNK>', 'not', 'on', 'as', 'if', 'with', 'of', '</s>']\n",
      "Trigram: ['<s>', 'they', 'all', 'went']\n"
     ]
    }
   ],
   "source": [
    "model = input(\"Which model would you like to use? (1) Vanilla (2) Laplace (3) UNK : \")\n",
    "phrase = input(\"Enter phrase to continue to generate: \")\n",
    "generate(model,phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generation_time():\n",
    "    generation_start = datetime.now()\n",
    "    \n",
    "    vanilla_uni(train_words)\n",
    "    vanilla_bi(train_words)\n",
    "    vanilla_tri(train_words)\n",
    "    \n",
    "    laplace_uni(train_words)\n",
    "    laplace_bi(train_words)\n",
    "    laplace_tri(train_words)\n",
    "    \n",
    "    unk_uni(train_words)\n",
    "    unk_bi(train_words)\n",
    "    unk_tri(train_words)\n",
    "    \n",
    "    generation_end = datetime.now()\n",
    "\n",
    "    generation_time = dict()\n",
    "    generation_time['generation_time'] = generation_end - generation_start\n",
    "    print('Generation Time(HH::MM:SS:ms) - {}\\n\\n'.format(generation_time['generation_time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RAMusage():\n",
    "    pid = os.getpid()\n",
    "    py = psutil.Process(pid)\n",
    "    memoryUse = py.memory_info()[0]/2.**30\n",
    "    print('Memory Use: ', memoryUse, 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of corpus: 76004 words\n",
      "Generation Time(HH::MM:SS:ms) - 0:00:00.387086\n",
      "\n",
      "\n",
      "Memory Use:  0.14656829833984375 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Total size of corpus: \"+str(len(tokens))+\" words\")\n",
    "model_generation_time()\n",
    "RAMusage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J. Daniel and J. Martin, “Speech and Language Processing,.” [Online]. Available: http://web.stanford.edu/~jurafsky/slp3/3.pdf.\n",
    "‌"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
